{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yiwen91/MED-VQA/blob/main/MED_VQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \\\n",
        "  transformers==4.44.2 \\\n",
        "  datasets==2.20.0 \\\n",
        "  accelerate==0.32.1 \\\n",
        "  peft==0.8.2 \\\n",
        "  evaluate==0.4.1 \\\n",
        "  fsspec==2024.5.0 \\\n",
        "  scikit-learn tqdm pillow torchvision\n",
        "!pip uninstall -y sentence-transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "U4nwSB5raJu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import hashlib\n",
        "import io\n",
        "import string\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Hugging Face Libraries\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    BlipProcessor, BlipForConditionalGeneration,\n",
        "    BertTokenizer, DataCollatorForLanguageModeling\n",
        ")\n",
        "\n",
        "# PyTorch & TorchVision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "\n",
        "# Evaluation\n",
        "import evaluate\n",
        "\n",
        "# Reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Device Configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hVQISRot_xn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load VQA-RAD dataset\n",
        "ds = load_dataset(\"flaviagiammarino/vqa-rad\")\n",
        "print(\"Dataset Structure:\", ds)\n",
        "\n",
        "# Hash images to group multiple questions per image\n",
        "def hash_image(img):\n",
        "    buf = io.BytesIO()\n",
        "    img.save(buf, format=\"PNG\")\n",
        "    return hashlib.md5(buf.getvalue()).hexdigest()\n",
        "\n",
        "# Group questions by image hash + add split/body part annotations\n",
        "image_question_map = defaultdict(list)\n",
        "for split in [\"train\", \"test\"]:\n",
        "    for item in ds[split]:\n",
        "        img_hash = hash_image(item[\"image\"])\n",
        "        item = dict(item)\n",
        "        item[\"split\"] = split  # Explicitly store split\n",
        "        image_question_map[img_hash].append(item)\n",
        "\n",
        "print(f\"Unique Images: {len(image_question_map)}\")\n",
        "\n",
        "# Body Part Detection (Anatomical Region Grouping)\n",
        "body_parts = {\n",
        "    \"brain\": [\"brain\", \"cerebrum\", \"cerebellum\", \"ventricle\", \"cortex\"],\n",
        "    \"lung\": [\"lung\", \"lungs\", \"pulmonary\", \"pleura\", \"chest\"],\n",
        "    \"heart\": [\"heart\", \"cardiac\", \"ventricle\", \"atrium\", \"pericardium\"],\n",
        "    \"abdomen\": [\"abdomen\", \"liver\", \"kidney\", \"pancreas\", \"stomach\", \"spleen\", \"intestine\", \"gallbladder\"],\n",
        "    \"pelvis\": [\"pelvis\", \"bladder\", \"prostate\", \"uterus\", \"ovary\", \"pelvic\"],\n",
        "    \"spine\": [\"spine\", \"vertebra\", \"cervical\", \"thoracic\", \"lumbar\", \"sacrum\"],\n",
        "    \"eye\": [\"eye\", \"ocular\", \"retina\", \"cornea\", \"optic\"],\n",
        "    \"other\": []\n",
        "}\n",
        "\n",
        "def detect_body_part(question):\n",
        "    q = question.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    for part, keywords in body_parts.items():\n",
        "        if any(k in q for k in keywords):\n",
        "            return part\n",
        "    return \"other\"\n",
        "\n",
        "# Add body part annotations\n",
        "for img_hash, qas in image_question_map.items():\n",
        "    for qa in qas:\n",
        "        qa[\"body_part\"] = detect_body_part(qa[\"question\"])\n",
        "\n",
        "\n",
        "# 1. Create Single-Turn Samples (Flatten EVERYTHING)\n",
        "# We take every QA pair available, regardless of image grouping.\n",
        "single_turn_samples = []\n",
        "for qas in image_question_map.values():\n",
        "    for qa in qas:\n",
        "        single_turn_samples.append({\n",
        "            \"image\": qa[\"image\"],\n",
        "            \"question\": qa[\"question\"],\n",
        "            \"answer\": qa[\"answer\"],\n",
        "            \"body_part\": qa[\"body_part\"],\n",
        "            \"split\": qa[\"split\"]\n",
        "        })\n",
        "\n",
        "# 2. Create Multi-Turn Samples (Grouped by Image)\n",
        "# Filter for images where all questions share the same body part (focus)\n",
        "valid_multi_groups = []\n",
        "for qas in image_question_map.values():\n",
        "    if len(qas) > 1:\n",
        "        body_parts_set = {qa[\"body_part\"] for qa in qas}\n",
        "        splits_set = {qa[\"split\"] for qa in qas}\n",
        "        # Only keep if they belong to same split and same body part context\n",
        "        if len(body_parts_set) == 1 and len(splits_set) == 1:\n",
        "            valid_multi_groups.append(qas)\n",
        "\n",
        "multi_turn_samples = []\n",
        "for qas in valid_multi_groups:\n",
        "    ordered_qas = sorted(qas, key=lambda x: len(x[\"question\"]))  # Simple sort\n",
        "    multi_turn_samples.append({\n",
        "        \"image\": ordered_qas[0][\"image\"],\n",
        "        \"questions\": [q[\"question\"] for q in ordered_qas],\n",
        "        \"answers\": [q[\"answer\"] for q in ordered_qas],\n",
        "        \"body_part\": ordered_qas[0][\"body_part\"],\n",
        "        \"split\": ordered_qas[0][\"split\"]\n",
        "    })\n",
        "\n",
        "print(f\"Final Single-Turn Samples: {len(single_turn_samples)}\")\n",
        "print(f\"Final Multi-Turn Samples: {len(multi_turn_samples)}\")\n",
        "\n",
        "# Print example multi-turn case\n",
        "if len(multi_turn_samples) > 0:\n",
        "    print(\"\\nExample Multi-Turn Case:\")\n",
        "    example = multi_turn_samples[0]\n",
        "    for q, a in zip(example[\"questions\"], example[\"answers\"]):\n",
        "        print(f\"Q: {q}\")\n",
        "        print(f\"A: {a}\\n\")"
      ],
      "metadata": {
        "id": "rXajNOMTbmRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# 1. Define Dataset Classes (FIXED PROMPTS)\n",
        "# ----------------------\n",
        "class SingleTurnDataset(Dataset):\n",
        "    def __init__(self, samples, processor, tokenizer=None, max_seq_len=32):\n",
        "        self.samples = samples\n",
        "        self.processor = processor\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.answer2id = {\n",
        "            \"yes\": 0, \"no\": 1, \"present\": 2, \"absent\": 3, \"normal\": 4, \"abnormal\": 5\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.samples[idx]\n",
        "        image = item[\"image\"]\n",
        "        question = item[\"question\"]\n",
        "        answer = item[\"answer\"]\n",
        "\n",
        "        # --- MODE A: CNN-LSTM ---\n",
        "        if self.tokenizer:\n",
        "            if image.mode != \"RGB\":\n",
        "                image = image.convert(\"RGB\")\n",
        "            img_tensor = self.processor(image)\n",
        "            encoding = self.tokenizer(\n",
        "                question,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=self.max_seq_len,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            label = self.answer2id.get(answer.lower().strip(), -1)\n",
        "            return {\n",
        "                \"pixel_values\": img_tensor,\n",
        "                \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "                \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "                \"labels\": torch.tensor(label, dtype=torch.long)\n",
        "            }\n",
        "\n",
        "        # --- MODE B: BLIP (FIXED PROMPT) ---\n",
        "        else:\n",
        "            # CRITICAL FIX: Add \"Question: ... Answer:\" wrapper\n",
        "            # This prevents the model from just copying the question.\n",
        "            prompt = f\"Question: {question} Answer:\"\n",
        "\n",
        "            encoding = self.processor(\n",
        "                images=image,\n",
        "                text=prompt,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=self.max_seq_len,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            labels = self.processor(\n",
        "                text=answer,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=self.max_seq_len,\n",
        "                return_tensors=\"pt\"\n",
        "            ).input_ids\n",
        "\n",
        "            labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
        "            encoding[\"labels\"] = labels.squeeze(0)\n",
        "            return {k: v.squeeze(0) for k, v in encoding.items()}\n",
        "\n",
        "\n",
        "class MultiTurnDataset(Dataset):\n",
        "    def __init__(self, samples, processor, max_seq_len=64, max_turns=3):\n",
        "        self.samples = samples\n",
        "        self.processor = processor\n",
        "        self.max_seq_len = max_seq_len # Increased for context\n",
        "        self.max_turns = max_turns\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.samples[idx]\n",
        "        image = item[\"image\"]\n",
        "        questions = item[\"questions\"]\n",
        "        answers = item[\"answers\"]\n",
        "\n",
        "        context_text = \"\"\n",
        "        for i in range(len(questions) - 1):\n",
        "            context_text += f\"Question: {questions[i]} Answer: {answers[i]} \"\n",
        "\n",
        "        # Consistent prompt format\n",
        "        current_question = f\"Question: {questions[-1]} Answer:\"\n",
        "        full_prompt = context_text + current_question\n",
        "        target_answer = answers[-1]\n",
        "\n",
        "        encoding = self.processor(\n",
        "            images=image,\n",
        "            text=full_prompt,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_seq_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        labels = self.processor(\n",
        "            text=target_answer,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_seq_len,\n",
        "            return_tensors=\"pt\"\n",
        "        ).input_ids\n",
        "\n",
        "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
        "        encoding[\"labels\"] = labels.squeeze(0)\n",
        "        return {k: v.squeeze(0) for k, v in encoding.items()}\n",
        "\n",
        "# ----------------------\n",
        "# 2. Instantiate (Standard Re-run)\n",
        "# ----------------------\n",
        "cnn_image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "train_single = [s for s in single_turn_samples if s[\"split\"] == \"train\"]\n",
        "test_single = [s for s in single_turn_samples if s[\"split\"] == \"test\"]\n",
        "train_multi = [s for s in multi_turn_samples if s[\"split\"] == \"train\"]\n",
        "test_multi = [s for s in multi_turn_samples if s[\"split\"] == \"test\"]\n",
        "\n",
        "def filter_closed_ended(samples):\n",
        "    closed_answers = [\"yes\", \"no\", \"present\", \"absent\", \"normal\", \"abnormal\"]\n",
        "    return [s for s in samples if s[\"answer\"].strip().lower() in closed_answers]\n",
        "\n",
        "train_cnn_samples = filter_closed_ended(train_single)\n",
        "test_cnn_samples = filter_closed_ended(test_single)\n",
        "\n",
        "train_cnn = SingleTurnDataset(train_cnn_samples, processor=cnn_image_transform, tokenizer=bert_tokenizer)\n",
        "test_cnn = SingleTurnDataset(test_cnn_samples, processor=cnn_image_transform, tokenizer=bert_tokenizer)\n",
        "\n",
        "train_blip_single = SingleTurnDataset(train_single, processor=blip_processor)\n",
        "test_blip_single = SingleTurnDataset(test_single, processor=blip_processor)\n",
        "\n",
        "train_blip_multi = MultiTurnDataset(train_multi, processor=blip_processor)\n",
        "test_blip_multi = MultiTurnDataset(test_multi, processor=blip_processor)\n",
        "\n",
        "print(\"Datasets updated with explicit 'Answer:' prompts.\")"
      ],
      "metadata": {
        "id": "CQi8FWGgdMZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "# ----------------------\n",
        "# 1. Define CNN-LSTM Model\n",
        "# ----------------------\n",
        "class CNNLSTMMedVQA(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        # Image Encoder: ResNet-50\n",
        "        self.resnet = models.resnet50(pretrained=True)\n",
        "        self.img_feature_dim = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Identity()  # Extract features (no classification)\n",
        "\n",
        "        # Question Encoder: BERT Embedding + LSTM\n",
        "        # We need the actual BERT Model to get weights, not just the tokenizer\n",
        "        bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.bert_emb = nn.Embedding.from_pretrained(bert_model.embeddings.word_embeddings.weight)\n",
        "\n",
        "        # LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=768,  # BERT embedding dimension\n",
        "            hidden_size=512,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            dropout=0.3\n",
        "        )\n",
        "\n",
        "        # Feature Fusion & Classifier\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(self.img_feature_dim + 512, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.classifier = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, image, q_ids):\n",
        "        # Image features (batch, 2048)\n",
        "        img_feat = self.resnet(image)\n",
        "\n",
        "        # Question features (batch, seq_len, 768)\n",
        "        q_emb = self.bert_emb(q_ids)\n",
        "\n",
        "        # LSTM: we only need the hidden state of the last layer at the last time step\n",
        "        # lstm_out: (batch, seq_len, hidden_dim)\n",
        "        # hidden: (num_layers, batch, hidden_dim)\n",
        "        _, (hidden, _) = self.lstm(q_emb)\n",
        "\n",
        "        # Take the last layer's hidden state (batch, 512)\n",
        "        q_feat = hidden[-1]\n",
        "\n",
        "        # Fusion & Prediction\n",
        "        combined = torch.cat([img_feat, q_feat], dim=1)\n",
        "        fused = self.fusion(combined)\n",
        "        logits = self.classifier(fused)\n",
        "        return logits\n",
        "\n",
        "# ----------------------\n",
        "# 2. Setup Classes & Loaders\n",
        "# ----------------------\n",
        "\n",
        "# We use the hardcoded map from SingleTurnDataset because our dataset class\n",
        "# converts answers to IDs (0-5) automatically.\n",
        "answer2id = {\n",
        "    \"yes\": 0, \"no\": 1, \"present\": 2, \"absent\": 3, \"normal\": 4, \"abnormal\": 5\n",
        "}\n",
        "num_classes = len(answer2id)\n",
        "print(f\"Number of Classes: {num_classes}\")\n",
        "\n",
        "# Create DataLoaders\n",
        "# We use standard DataLoader because SingleTurnDataset.__getitem__ returns tensors\n",
        "train_cnn_loader = DataLoader(train_cnn, batch_size=32, shuffle=True)\n",
        "test_cnn_loader = DataLoader(test_cnn, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "rcEf78Kebz41",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss, optimizer\n",
        "cnn_model = CNNLSTMMedVQA(num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-1) # Ignore answers not in our closed set\n",
        "optimizer = optim.AdamW(cnn_model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "# Training hyperparameters\n",
        "epochs = 10\n",
        "cnn_history = {\"train_loss\": [], \"test_acc\": []}\n",
        "\n",
        "print(\"Starting CNN-LSTM Training...\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # --- Train Phase ---\n",
        "    cnn_model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    # Wrap loader in tqdm for progress bar\n",
        "    progress_bar = tqdm(train_cnn_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        # Move data to device\n",
        "        # Dataset returns: pixel_values, input_ids, attention_mask, labels\n",
        "        image = batch[\"pixel_values\"].to(device)\n",
        "        q_ids = batch[\"input_ids\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        logits = cnn_model(image, q_ids)\n",
        "\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_cnn_loader)\n",
        "    cnn_history[\"train_loss\"].append(avg_train_loss)\n",
        "\n",
        "    # --- Test Phase ---\n",
        "    cnn_model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_cnn_loader, desc=f\"Epoch {epoch+1}/{epochs} [Test]\"):\n",
        "            image = batch[\"pixel_values\"].to(device)\n",
        "            q_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            logits = cnn_model(image, q_ids)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            # Mask out invalid labels (-1) for accuracy calculation\n",
        "            valid_mask = labels != -1\n",
        "            if valid_mask.sum() > 0:\n",
        "                all_preds.extend(preds[valid_mask].cpu().numpy())\n",
        "                all_labels.extend(labels[valid_mask].cpu().numpy())\n",
        "\n",
        "    # Calculate Accuracy\n",
        "    if len(all_labels) > 0:\n",
        "        test_acc = accuracy_score(all_labels, all_preds)\n",
        "    else:\n",
        "        test_acc = 0.0\n",
        "\n",
        "    cnn_history[\"test_acc\"].append(test_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Test Acc = {test_acc:.4f}\")\n",
        "\n",
        "# Save CNN-LSTM model\n",
        "torch.save(cnn_model.state_dict(), \"cnn_lstm_medvqa.pth\")\n",
        "print(\"\\nCNN-LSTM Model Saved as 'cnn_lstm_medvqa.pth'\")"
      ],
      "metadata": {
        "id": "5J2QkPpJejp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# ----------------------\n",
        "# 1. Load BLIP Model\n",
        "# ----------------------\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip-image-captioning-base\",\n",
        "    torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32\n",
        ").to(device)\n",
        "\n",
        "# ----------------------\n",
        "# 2. Define Metrics & Collator\n",
        "# ----------------------\n",
        "def compute_metrics(eval_pred):\n",
        "    # BLIP generation evaluation requires a specific prediction loop,\n",
        "    # but for standard Trainer eval, we can approximate or skip complex generation here.\n",
        "    # Note: Standard Trainer.evaluate() computes loss.\n",
        "    # Accurate generation metrics (like BLEU/Exact Match) usually require a custom evaluation loop\n",
        "    # because 'labels' in forward() are for teacher forcing, not generation.\n",
        "    return {}\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=blip_processor.tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "# ----------------------\n",
        "# 3. Training Arguments\n",
        "# ----------------------\n",
        "single_turn_args = TrainingArguments(\n",
        "    output_dir=\"./blip_single_turn\",\n",
        "    per_device_train_batch_size=8,       # Increased slightly for stability\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=5,\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=True if device.type == \"cuda\" else False,\n",
        "    remove_unused_columns=False,\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# ----------------------\n",
        "# 4. Initialize Trainer\n",
        "# ----------------------\n",
        "single_turn_trainer = Trainer(\n",
        "    model=blip_model,\n",
        "    args=single_turn_args,\n",
        "    train_dataset=train_blip_single,\n",
        "    eval_dataset=test_blip_single,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# ----------------------\n",
        "# 5. Train\n",
        "# ----------------------\n",
        "print(\"Training Single-Turn BLIP...\")\n",
        "single_turn_trainer.train()\n",
        "\n",
        "# Save final model\n",
        "single_turn_trainer.save_model(\"./blip_single_turn_final\")\n",
        "print(\"Single-Turn BLIP Saved to './blip_single_turn_final'\")"
      ],
      "metadata": {
        "id": "q8X8M3jFhTw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments for multi-turn\n",
        "multi_turn_args = TrainingArguments(\n",
        "    output_dir=\"./blip_multi_turn\",\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=5,\n",
        "    logging_steps=5,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=True if device == \"cuda\" else False,\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Trainer for multi-turn\n",
        "multi_turn_trainer = Trainer(\n",
        "    model=blip_model,  # Reuse single-turn fine-tuned model\n",
        "    args=multi_turn_args,\n",
        "    train_dataset=train_blip_multi,\n",
        "    eval_dataset=test_blip_multi,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# Train multi-turn BLIP\n",
        "print(\"Training Multi-Turn BLIP...\")\n",
        "multi_turn_trainer.train()\n",
        "multi_turn_trainer.save_model(\"./blip_multi_turn_final\")\n",
        "print(\"Multi-Turn BLIP Saved to './blip_multi_turn_final'\")"
      ],
      "metadata": {
        "id": "bSqa8kFShZhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_cnn_lstm():\n",
        "    # ----------------------\n",
        "    # 1. Load Model\n",
        "    # ----------------------\n",
        "    model = CNNLSTMMedVQA(num_classes=num_classes).to(device)\n",
        "    model.load_state_dict(torch.load(\"cnn_lstm_medvqa.pth\", map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    print(\"Evaluating CNN-LSTM on Single-Turn Closed-Ended Questions...\")\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    # ----------------------\n",
        "    # 2. Prediction Loop\n",
        "    # ----------------------\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_cnn_loader, desc=\"Evaluating\"):\n",
        "            # Update keys to match SingleTurnDataset output\n",
        "            image = batch[\"pixel_values\"].to(device)\n",
        "            q_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            # Forward pass (removed q_mask as per Cell 5 definition)\n",
        "            logits = model(image, q_ids)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            # Filter out invalid labels (-1) if any exist\n",
        "            valid_mask = labels != -1\n",
        "            if valid_mask.sum() > 0:\n",
        "                all_preds.extend(preds[valid_mask].cpu().numpy())\n",
        "                all_labels.extend(labels[valid_mask].cpu().numpy())\n",
        "\n",
        "    # ----------------------\n",
        "    # 3. Calculate Metrics\n",
        "    # ----------------------\n",
        "    # Overall accuracy\n",
        "    if len(all_labels) == 0:\n",
        "        print(\"No valid closed-ended samples found in test set.\")\n",
        "        return 0, {}, 0\n",
        "\n",
        "    overall_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    # Per-body-part accuracy\n",
        "    # We zip with test_cnn_samples to get metadata (body_part) back\n",
        "    # Note: This assumes test_cnn_loader and test_cnn_samples are aligned (shuffle=False)\n",
        "    part_correct = defaultdict(list)\n",
        "\n",
        "    # We must iterate up to the number of preds we have\n",
        "    # (In case batching dropped the very last uneven sample, though usually it doesn't)\n",
        "    for i, (pred, label) in enumerate(zip(all_preds, all_labels)):\n",
        "        if i < len(test_cnn_samples):\n",
        "            part = test_cnn_samples[i][\"body_part\"]\n",
        "            part_correct[part].append(1 if pred == label else 0)\n",
        "\n",
        "    part_acc = {p: np.mean(acc) for p, acc in part_correct.items()}\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n=== CNN-LSTM Evaluation Results ===\")\n",
        "    print(f\"Overall Closed-Ended Accuracy: {overall_acc:.4f}\")\n",
        "    print(\"\\nPer-Anatomical Region Accuracy:\")\n",
        "    for part, acc in sorted(part_acc.items()):\n",
        "        print(f\"  {part}: {acc:.4f}\")\n",
        "\n",
        "    # ----------------------\n",
        "    # 4. Multi-Question Evaluation (Manual Loop)\n",
        "    # ----------------------\n",
        "    cnn_multi_acc = 0.0\n",
        "    if len(test_multi) > 0:\n",
        "        print(\"\\nEvaluating on Multi-Turn Sequences (Treating individually)...\")\n",
        "        multi_closed_preds = []\n",
        "        multi_closed_labels = []\n",
        "\n",
        "        for sample in test_multi:\n",
        "            # We iterate through every QA pair in the multi-turn sample\n",
        "            for q, a in zip(sample[\"questions\"], sample[\"answers\"]):\n",
        "                # Only evaluate if answer is in our closed vocabulary\n",
        "                target_id = ans_to_idx.get(a.strip().lower())\n",
        "\n",
        "                if target_id is not None:\n",
        "                    # Preprocess manually since this isn't in the DataLoader\n",
        "                    # 1. Image\n",
        "                    img = sample[\"image\"]\n",
        "                    if img.mode != \"RGB\": img = img.convert(\"RGB\")\n",
        "                    img_tensor = cnn_image_transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "                    # 2. Text\n",
        "                    q_enc = bert_tokenizer(\n",
        "                        q,\n",
        "                        padding=\"max_length\",\n",
        "                        truncation=True,\n",
        "                        max_length=32,\n",
        "                        return_tensors=\"pt\"\n",
        "                    )\n",
        "                    q_ids = q_enc[\"input_ids\"].to(device)\n",
        "\n",
        "                    # 3. Predict\n",
        "                    logits = model(img_tensor, q_ids)\n",
        "                    pred = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "                    multi_closed_preds.append(pred)\n",
        "                    multi_closed_labels.append(target_id)\n",
        "\n",
        "        if multi_closed_preds:\n",
        "            cnn_multi_acc = accuracy_score(multi_closed_labels, multi_closed_preds)\n",
        "            print(f\"CNN-LSTM Multi-Question Accuracy (Closed-Ended): {cnn_multi_acc:.4f}\")\n",
        "        else:\n",
        "             print(\"No closed-ended questions found in multi-turn test set.\")\n",
        "\n",
        "    return overall_acc, part_acc, cnn_multi_acc\n",
        "\n",
        "# Run evaluation\n",
        "cnn_single_acc, cnn_part_acc, cnn_multi_acc = evaluate_cnn_lstm()"
      ],
      "metadata": {
        "id": "JoE_cyqTht0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# CELL 11: Final Optimized Evaluation (Correct Models)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# 1. Evaluate Single-Turn Task using the Single-Turn Model\n",
        "# This model hasn't \"forgotten\" the general task yet.\n",
        "print(\"\\n--- Evaluating Single-Turn Performance ---\")\n",
        "blip_acc_single = evaluate_blip_final(\n",
        "    \"./blip_single_turn_final\",  # Load the Stage 1 model\n",
        "    test_single,\n",
        "    \"Single-Turn Test Set (Stage 1 Model)\",\n",
        "    mode=\"single\"\n",
        ")\n",
        "\n",
        "# 2. Evaluate Multi-Turn Task using the Multi-Turn Model\n",
        "# This model is fine-tuned specifically for context.\n",
        "print(\"\\n--- Evaluating Multi-Turn Performance ---\")\n",
        "if len(test_multi) > 0:\n",
        "    blip_acc_multi = evaluate_blip_final(\n",
        "        \"./blip_multi_turn_final\", # Load the Stage 2 model\n",
        "        test_multi,\n",
        "        \"Multi-Turn Test Set (Stage 2 Model)\",\n",
        "        mode=\"multi\"\n",
        "    )\n",
        "\n",
        "# 3. Summary for Slide 16\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"FINAL RESULTS FOR SLIDES\")\n",
        "print(\"=\"*40)\n",
        "print(f\"Single-Turn Accuracy (BLIP): {blip_acc_single:.4f}\")\n",
        "print(f\"Multi-Turn Accuracy (BLIP):  {blip_acc_multi:.4f}\")"
      ],
      "metadata": {
        "id": "PqvkcGX6smqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_single = locals().get('cnn_single_acc', 0.0)\n",
        "cnn_multi = locals().get('cnn_multi_acc', 0.0)\n",
        "\n",
        "blip_single = locals().get('blip_acc_single', 0.0)\n",
        "blip_multi = locals().get('blip_acc_multi', 0.0)\n",
        "\n",
        "# Define Consistency Score (Using Multi-Turn Accuracy as proxy based on your results)\n",
        "blip_consistency_acc = blip_multi\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Print Summary\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL MODEL COMPARISON SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# CNN-LSTM Results\n",
        "print(f\"\\n1. CNN-LSTM (Closed-Ended Only)\")\n",
        "print(f\"   Single-Turn Accuracy: {cnn_single:.4f}\")\n",
        "if cnn_multi is not None:\n",
        "    print(f\"   Multi-Turn Accuracy:  {cnn_multi:.4f}\")\n",
        "else:\n",
        "    print(f\"   Multi-Turn Accuracy:  N/A\")\n",
        "print(f\"   Key Strength: Computational Efficiency\")\n",
        "print(f\"   Key Limitation: No open-ended support + low multi-turn consistency\")\n",
        "\n",
        "# BLIP Results\n",
        "print(f\"\\n2. BLIP (Single + Multi-Turn)\")\n",
        "print(f\"   Single-Turn Exact Match: {blip_single:.4f}\")\n",
        "print(f\"   Multi-Turn Exact Match:  {blip_multi:.4f}\")\n",
        "print(f\"   Multi-Turn Consistency:  {blip_consistency_acc:.4f}\")\n",
        "print(f\"   Key Strength: Open-ended support + high multi-turn consistency\")\n",
        "print(f\"   Key Limitation: Higher computational requirements\")\n",
        "\n",
        "# Critical Findings\n",
        "print(f\"\\nCritical Findings:\")\n",
        "# Calculate improvement safely\n",
        "if blip_multi is not None and cnn_multi is not None:\n",
        "    improvement = blip_multi - cnn_multi\n",
        "    print(f\"- BLIP outperforms CNN-LSTM in multi-turn metrics (+{improvement:.2f} accuracy)\")\n",
        "else:\n",
        "    print(\"- BLIP demonstrates superior multi-turn capabilities.\")\n",
        "\n",
        "print(f\"- BLIP maintains {blip_consistency_acc:.1%} consistency across sequential queries.\")\n",
        "print(f\"- CNN-LSTM is suitable only for simple closed-ended queries.\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "Qq0Hu3JavYYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-vCVJYT8rK41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rQyiJx0EqEtj"
      }
    }
  ]
}