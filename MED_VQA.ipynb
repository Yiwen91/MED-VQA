{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yiwen91/MED-VQA/blob/main/MED_VQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \\\n",
        "  transformers==4.44.2 \\\n",
        "  datasets==2.20.0 \\\n",
        "  accelerate==0.32.1 \\\n",
        "  peft==0.8.2 \\\n",
        "  evaluate==0.4.1 \\\n",
        "  fsspec==2024.5.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "U4nwSB5raJu0",
        "outputId": "29ee24e0-6954-45d2-db21-44e172e7c523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.44.2 in /usr/local/lib/python3.12/dist-packages (4.44.2)\n",
            "Requirement already satisfied: datasets==2.20.0 in /usr/local/lib/python3.12/dist-packages (2.20.0)\n",
            "Requirement already satisfied: accelerate==0.32.1 in /usr/local/lib/python3.12/dist-packages (0.32.1)\n",
            "Requirement already satisfied: peft==0.8.2 in /usr/local/lib/python3.12/dist-packages (0.8.2)\n",
            "Requirement already satisfied: evaluate==0.4.1 in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: fsspec==2024.5.0 in /usr/local/lib/python3.12/dist-packages (2024.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (3.20.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (0.7.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (18.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (0.7)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (3.13.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==0.32.1) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.32.1) (2.9.0+cpu)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.12/dist-packages (from evaluate==0.4.1) (0.18.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (1.22.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.44.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.44.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.44.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.44.2) (2026.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.32.1) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.32.1) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.32.1) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.32.1) (3.1.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.20.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.20.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.20.0) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.20.0) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.10.0->accelerate==0.32.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.32.1) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip-image-captioning-base\",\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "print(\"BLIP loaded on\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hVQISRot_xn1",
        "outputId": "b47800dd-9f79-47b6-c610-d213dddded2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLIP loaded on cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import io\n",
        "import string\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Hugging Face\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    BlipProcessor,\n",
        "    BlipForConditionalGeneration\n",
        ")\n",
        "\n",
        "# PyTorch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Hugging Face Transformers\n",
        "from transformers import BertTokenizer, Blip2Processor\n",
        "\n",
        "# TorchVision & PyTorch for CNN-LSTM preprocessing\n",
        "from torchvision import transforms\n",
        "\n",
        "# Evaluation\n",
        "import evaluate\n",
        "\n",
        "# Reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n"
      ],
      "metadata": {
        "id": "rXajNOMTbmRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "import io\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "CQi8FWGgdMZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load VQA-RAD dataset\n",
        "ds = load_dataset(\"flaviagiammarino/vqa-rad\")\n",
        "print(\"Dataset Structure:\", ds)\n",
        "print(\"Sample Train Item:\", ds[\"train\"][0])\n",
        "\n",
        "# Hash images to group multiple questions per image (for multi-turn setup)\n",
        "def hash_image(img):\n",
        "    buf = io.BytesIO()\n",
        "    img.save(buf, format=\"PNG\")\n",
        "    return hashlib.md5(buf.getvalue()).hexdigest()\n",
        "\n",
        "image_question_map = defaultdict(list)\n",
        "for split in [\"train\", \"test\"]:\n",
        "    for item in ds[split]:\n",
        "        img_hash = hash_image(item[\"image\"])\n",
        "        image_question_map[img_hash].append(item)\n",
        "\n",
        "print(f\"Unique Images: {len(image_question_map)}\")\n",
        "\n",
        "# Body Part Detection (Anatomical Region Grouping, as in Preliminary Results)\n",
        "body_parts = {\n",
        "    \"brain\": [\"brain\", \"cerebrum\", \"cerebellum\", \"ventricle\", \"cortex\"],\n",
        "    \"lung\": [\"lung\", \"lungs\", \"pulmonary\", \"pleura\", \"chest\"],\n",
        "    \"heart\": [\"heart\", \"cardiac\", \"ventricle\", \"atrium\", \"pericardium\"],\n",
        "    \"abdomen\": [\"abdomen\", \"liver\", \"kidney\", \"pancreas\", \"stomach\", \"spleen\", \"intestine\", \"gallbladder\"],\n",
        "    \"pelvis\": [\"pelvis\", \"bladder\", \"prostate\", \"uterus\", \"ovary\", \"pelvic\"],\n",
        "    \"spine\": [\"spine\", \"vertebra\", \"cervical\", \"thoracic\", \"lumbar\", \"sacrum\"],\n",
        "    \"eye\": [\"eye\", \"ocular\", \"retina\", \"cornea\", \"optic\"],\n",
        "    \"other\": []\n",
        "}\n",
        "\n",
        "def detect_body_part(question):\n",
        "    q = question.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    for part, keywords in body_parts.items():\n",
        "        if any(k in q for k in keywords):\n",
        "            return part\n",
        "    return \"other\"\n",
        "\n",
        "# Add body part annotations to all samples\n",
        "for img_hash, qas in image_question_map.items():\n",
        "    for qa in qas:\n",
        "        qa[\"body_part\"] = detect_body_part(qa[\"question\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcEf78Kebz41",
        "outputId": "2e0130c0-6f59-445e-d966-908c8f020db1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Structure: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['image', 'question', 'answer'],\n",
            "        num_rows: 1793\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['image', 'question', 'answer'],\n",
            "        num_rows: 451\n",
            "    })\n",
            "})\n",
            "Sample Train Item: {'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=566x555 at 0x7BC50177BC50>, 'question': 'are regions of the brain infarcted?', 'answer': 'yes'}\n",
            "Unique Images: 314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Image Hashing & Multi-turn Construction\n",
        "# ===============================\n",
        "\n",
        "import io\n",
        "import hashlib\n",
        "import string\n",
        "from collections import defaultdict\n",
        "\n",
        "# Hash images to group multiple questions per image\n",
        "def hash_image(img):\n",
        "    buf = io.BytesIO()\n",
        "    img.save(buf, format=\"PNG\")\n",
        "    return hashlib.md5(buf.getvalue()).hexdigest()\n",
        "\n",
        "# -------------------------------\n",
        "# Group questions by image hash\n",
        "# -------------------------------\n",
        "image_question_map = defaultdict(list)\n",
        "\n",
        "for split in [\"train\", \"test\"]:\n",
        "    for item in ds[split]:\n",
        "        img_hash = hash_image(item[\"image\"])\n",
        "        item = dict(item)          # avoid modifying HF dataset object\n",
        "        item[\"split\"] = split      # IMPORTANT: store split explicitly\n",
        "        image_question_map[img_hash].append(item)\n",
        "\n",
        "print(f\"Unique Images: {len(image_question_map)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Body Part Detection\n",
        "# -------------------------------\n",
        "body_parts = {\n",
        "    \"brain\": [\"brain\", \"cerebrum\", \"cerebellum\", \"ventricle\", \"cortex\"],\n",
        "    \"lung\": [\"lung\", \"lungs\", \"pulmonary\", \"pleura\", \"chest\"],\n",
        "    \"heart\": [\"heart\", \"cardiac\", \"ventricle\", \"atrium\", \"pericardium\"],\n",
        "    \"abdomen\": [\"abdomen\", \"liver\", \"kidney\", \"pancreas\", \"stomach\", \"spleen\", \"intestine\", \"gallbladder\"],\n",
        "    \"pelvis\": [\"pelvis\", \"bladder\", \"prostate\", \"uterus\", \"ovary\", \"pelvic\"],\n",
        "    \"spine\": [\"spine\", \"vertebra\", \"cervical\", \"thoracic\", \"lumbar\", \"sacrum\"],\n",
        "    \"eye\": [\"eye\", \"ocular\", \"retina\", \"cornea\", \"optic\"],\n",
        "    \"other\": []\n",
        "}\n",
        "\n",
        "def detect_body_part(question):\n",
        "    q = question.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    for part, keywords in body_parts.items():\n",
        "        if any(k in q for k in keywords):\n",
        "            return part\n",
        "    return \"other\"\n",
        "\n",
        "# Add body part annotation\n",
        "for img_hash, qas in image_question_map.items():\n",
        "    for qa in qas:\n",
        "        qa[\"body_part\"] = detect_body_part(qa[\"question\"])\n",
        "\n",
        "# -------------------------------\n",
        "# Split into single / multi Q\n",
        "# -------------------------------\n",
        "single_q = []\n",
        "multi_q = []\n",
        "\n",
        "for qas in image_question_map.values():\n",
        "    if len(qas) == 1:\n",
        "        single_q.append(qas[0])\n",
        "    else:\n",
        "        multi_q.append(qas)\n",
        "\n",
        "print(f\"Single-Question Images: {len(single_q)}\")\n",
        "print(f\"Multi-Question Images: {len(multi_q)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Keep valid multi-turn samples\n",
        "# (same body part + same split)\n",
        "# -------------------------------\n",
        "valid_multi = []\n",
        "\n",
        "for qas in multi_q:\n",
        "    body_parts_set = {qa[\"body_part\"] for qa in qas}\n",
        "    splits_set = {qa[\"split\"] for qa in qas}\n",
        "    if len(body_parts_set) == 1 and len(splits_set) == 1:\n",
        "        valid_multi.append(qas)\n",
        "\n",
        "print(f\"Valid Multi-Question Cases: {len(valid_multi)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Create final samples\n",
        "# -------------------------------\n",
        "def create_single_turn_samples():\n",
        "    samples = []\n",
        "    for qa in single_q:\n",
        "        samples.append({\n",
        "            \"image\": qa[\"image\"],\n",
        "            \"question\": qa[\"question\"],\n",
        "            \"answer\": qa[\"answer\"],\n",
        "            \"body_part\": qa[\"body_part\"],\n",
        "            \"split\": qa[\"split\"]\n",
        "        })\n",
        "    return samples\n",
        "\n",
        "def create_multi_turn_samples():\n",
        "    samples = []\n",
        "    for qas in valid_multi:\n",
        "        ordered_qas = sorted(qas, key=lambda x: len(x[\"question\"]))\n",
        "        samples.append({\n",
        "            \"image\": ordered_qas[0][\"image\"],\n",
        "            \"questions\": [q[\"question\"] for q in ordered_qas],\n",
        "            \"answers\": [q[\"answer\"] for q in ordered_qas],\n",
        "            \"body_part\": ordered_qas[0][\"body_part\"],\n",
        "            \"split\": ordered_qas[0][\"split\"]\n",
        "        })\n",
        "    return samples\n",
        "\n",
        "single_turn_samples = create_single_turn_samples()\n",
        "multi_turn_samples = create_multi_turn_samples()\n",
        "\n",
        "print(f\"Final Single-Turn Samples: {len(single_turn_samples)}\")\n",
        "print(f\"Final Multi-Turn Samples: {len(multi_turn_samples)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Example multi-turn case\n",
        "# -------------------------------\n",
        "print(\"\\nExample Multi-Turn Case:\")\n",
        "example = multi_turn_samples[0]\n",
        "for q, a in zip(example[\"questions\"], example[\"answers\"]):\n",
        "    print(f\"Q: {q}\")\n",
        "    print(f\"A: {a}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5J2QkPpJejp_",
        "outputId": "0e253db6-2537-40f3-dec0-f36035c99deb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Images: 314\n",
            "Single-Question Images: 0\n",
            "Multi-Question Images: 314\n",
            "Valid Multi-Question Cases: 37\n",
            "Final Single-Turn Samples: 0\n",
            "Final Multi-Turn Samples: 37\n",
            "\n",
            "Example Multi-Turn Case:\n",
            "Q: where is the mass?\n",
            "A: left temporal horn\n",
            "\n",
            "Q: how would you describe the mass?\n",
            "A: isointense\n",
            "\n",
            "Q: is there a fracture of the skull?\n",
            "A: no\n",
            "\n",
            "Q: what is the location of the mass?\n",
            "A: left temporal horn\n",
            "\n",
            "Q: what are the characteristics of the mass?\n",
            "A: isointense\n",
            "\n",
            "Q: are there other abnormalities besides the mass in the temporal horn?\n",
            "A: yes\n",
            "\n",
            "Q: besides the mass in the temporal horn, are there other enhancements in the image?\n",
            "A: yes\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Split into single- and multi-question groups\n",
        "# ===============================\n",
        "\n",
        "single_q = []\n",
        "multi_q = []\n",
        "\n",
        "for img_hash, qas in image_question_map.items():\n",
        "    if len(qas) == 1:\n",
        "        single_q.append(qas[0])\n",
        "    else:\n",
        "        multi_q.append(qas)\n",
        "\n",
        "print(f\"Single-Question Images: {len(single_q)}\")\n",
        "print(f\"Multi-Question Images: {len(multi_q)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Filter valid multi-question cases\n",
        "# (same body part + same split)\n",
        "# -------------------------------\n",
        "valid_multi = []\n",
        "\n",
        "for qas in multi_q:\n",
        "    body_parts = {qa[\"body_part\"] for qa in qas}\n",
        "    splits = {qa[\"split\"] for qa in qas}   # IMPORTANT FIX\n",
        "\n",
        "    if len(body_parts) == 1 and len(splits) == 1:\n",
        "        valid_multi.append(qas)\n",
        "\n",
        "print(f\"Valid Multi-Question Cases: {len(valid_multi)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Create structured samples\n",
        "# -------------------------------\n",
        "def create_single_turn_samples():\n",
        "    samples = []\n",
        "    for qa in single_q:\n",
        "        samples.append({\n",
        "            \"image\": qa[\"image\"],\n",
        "            \"question\": qa[\"question\"],\n",
        "            \"answer\": qa[\"answer\"],\n",
        "            \"body_part\": qa[\"body_part\"],\n",
        "            \"split\": qa[\"split\"]   # FIXED\n",
        "        })\n",
        "    return samples\n",
        "\n",
        "def create_multi_turn_samples():\n",
        "    samples = []\n",
        "    for qas in valid_multi:\n",
        "        ordered_qas = sorted(qas, key=lambda x: len(x[\"question\"]))\n",
        "        samples.append({\n",
        "            \"image\": ordered_qas[0][\"image\"],\n",
        "            \"questions\": [q[\"question\"] for q in ordered_qas],\n",
        "            \"answers\": [q[\"answer\"] for q in ordered_qas],\n",
        "            \"body_part\": ordered_qas[0][\"body_part\"],\n",
        "            \"split\": ordered_qas[0][\"split\"]   # FIXED\n",
        "        })\n",
        "    return samples\n",
        "\n",
        "single_turn_samples = create_single_turn_samples()\n",
        "multi_turn_samples = create_multi_turn_samples()\n",
        "\n",
        "print(f\"Final Single-Turn Samples: {len(single_turn_samples)}\")\n",
        "print(f\"Final Multi-Turn Samples: {len(multi_turn_samples)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Example multi-turn case\n",
        "# -------------------------------\n",
        "print(\"\\nExample Multi-Turn Case:\")\n",
        "example = multi_turn_samples[0]\n",
        "for q, a in zip(example[\"questions\"], example[\"answers\"]):\n",
        "    print(f\"Q: {q}\")\n",
        "    print(f\"A: {a}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8X8M3jFhTw5",
        "outputId": "2c28d66b-fcac-4af2-cf5c-30186ad9dc48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single-Question Images: 0\n",
            "Multi-Question Images: 314\n",
            "Valid Multi-Question Cases: 37\n",
            "Final Single-Turn Samples: 0\n",
            "Final Multi-Turn Samples: 37\n",
            "\n",
            "Example Multi-Turn Case:\n",
            "Q: where is the mass?\n",
            "A: left temporal horn\n",
            "\n",
            "Q: how would you describe the mass?\n",
            "A: isointense\n",
            "\n",
            "Q: is there a fracture of the skull?\n",
            "A: no\n",
            "\n",
            "Q: what is the location of the mass?\n",
            "A: left temporal horn\n",
            "\n",
            "Q: what are the characteristics of the mass?\n",
            "A: isointense\n",
            "\n",
            "Q: are there other abnormalities besides the mass in the temporal horn?\n",
            "A: yes\n",
            "\n",
            "Q: besides the mass in the temporal horn, are there other enhancements in the image?\n",
            "A: yes\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleTurnDataset(Dataset):\n",
        "    def __init__(self, samples, processor=None, tokenizer=None, max_seq_len=32):\n",
        "        self.samples = samples\n",
        "        self.processor = processor  # For BLIP-2\n",
        "        self.tokenizer = tokenizer  # For CNN-LSTM (text)\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        image = sample[\"image\"]\n",
        "        question = sample[\"question\"]\n",
        "        answer = sample[\"answer\"]\n",
        "\n",
        "        if self.processor:  # BLIP-2\n",
        "            inputs = self.processor(\n",
        "                images=image,\n",
        "                text=question,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=self.max_seq_len\n",
        "            )\n",
        "            inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
        "            inputs[\"labels\"] = self.processor.tokenizer(\n",
        "                answer,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=16\n",
        "            )[\"input_ids\"].squeeze(0)\n",
        "            return inputs\n",
        "\n",
        "        elif self.tokenizer:  # CNN-LSTM\n",
        "            q_enc = self.tokenizer(\n",
        "                question,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=self.max_seq_len,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            img_tensor = self.processor(image).unsqueeze(0)  # image transform\n",
        "            return {\n",
        "                \"image\": img_tensor.squeeze(0),\n",
        "                \"q_ids\": q_enc[\"input_ids\"].squeeze(0),\n",
        "                \"q_mask\": q_enc[\"attention_mask\"].squeeze(0),\n",
        "                \"answer\": answer,\n",
        "                \"body_part\": sample[\"body_part\"]\n",
        "            }\n",
        "\n",
        "\n",
        "class MultiTurnDataset(Dataset):\n",
        "    def __init__(self, samples, processor, max_seq_len=32, max_turns=3):\n",
        "        self.samples = samples\n",
        "        self.processor = processor\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.max_turns = max_turns\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        image = sample[\"image\"]\n",
        "        questions = sample[\"questions\"][:self.max_turns]\n",
        "        answers = sample[\"answers\"][:self.max_turns]\n",
        "\n",
        "        # Build conversation history\n",
        "        conversation = \"\"\n",
        "        if len(questions) > 1:\n",
        "            for q, a in zip(questions[:-1], answers[:-1]):\n",
        "                conversation += f\"Q: {q} A: {a} \"\n",
        "        conversation += f\"Q: {questions[-1]} A:\"\n",
        "\n",
        "        # Encode for BLIP-2\n",
        "        inputs = self.processor(\n",
        "            images=image,\n",
        "            text=conversation,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=min(self.max_seq_len * self.max_turns, 512)\n",
        "        )\n",
        "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
        "        # Encode target answer\n",
        "        inputs[\"labels\"] = self.processor.tokenizer(\n",
        "            answers[-1],\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=16\n",
        "        )[\"input_ids\"].squeeze(0)\n",
        "        return inputs\n"
      ],
      "metadata": {
        "id": "bSqa8kFShZhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN-LSTM preprocessors\n",
        "cnn_image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# BLIP-1 preprocessor\n",
        "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "# Split train/test data\n",
        "train_single = [s for s in single_turn_samples if s[\"split\"] == \"train\"]\n",
        "test_single = [s for s in single_turn_samples if s[\"split\"] == \"test\"]\n",
        "train_multi = [s for s in multi_turn_samples if s[\"split\"] == \"train\"]\n",
        "test_multi = [s for s in multi_turn_samples if s[\"split\"] == \"test\"]\n",
        "\n",
        "print(f\"Single-Turn: Train={len(train_single)}, Test={len(test_single)}\")\n",
        "print(f\"Multi-Turn: Train={len(train_multi)}, Test={len(test_multi)}\")\n",
        "\n",
        "# Filter closed-ended questions for CNN-LSTM\n",
        "def filter_closed_ended(samples):\n",
        "    closed_answers = [\"yes\", \"no\", \"present\", \"absent\", \"normal\", \"abnormal\"]\n",
        "    return [s for s in samples if s[\"answer\"].strip().lower() in closed_answers]\n",
        "\n",
        "# Create datasets\n",
        "train_cnn = SingleTurnDataset(\n",
        "    filter_closed_ended(train_single),\n",
        "    processor=cnn_image_transform,\n",
        "    tokenizer=bert_tokenizer,\n",
        "    max_seq_len=32\n",
        ")\n",
        "test_cnn = SingleTurnDataset(\n",
        "    filter_closed_ended(test_single),\n",
        "    processor=cnn_image_transform,\n",
        "    tokenizer=bert_tokenizer,\n",
        "    max_seq_len=32\n",
        ")\n",
        "\n",
        "train_blip_single = SingleTurnDataset(\n",
        "    train_single,\n",
        "    processor=blip_processor,\n",
        "    max_seq_len=32\n",
        ")\n",
        "test_blip_single = SingleTurnDataset(\n",
        "    test_single,\n",
        "    processor=blip_processor,\n",
        "    max_seq_len=32\n",
        ")\n",
        "\n",
        "train_blip_multi = MultiTurnDataset(\n",
        "    train_multi,\n",
        "    processor=blip_processor,\n",
        "    max_seq_len=32,\n",
        "    max_turns=3\n",
        ")\n",
        "test_blip_multi = MultiTurnDataset(\n",
        "    test_multi,\n",
        "    processor=blip_processor,\n",
        "    max_seq_len=32,\n",
        "    max_turns=3\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoE_cyqTht0p",
        "outputId": "b57b3ae8-62b1-40d6-ec91-22bfbb375c67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single-Turn: Train=0, Test=0\n",
            "Multi-Turn: Train=36, Test=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN-LSTM preprocessors\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Image transform for CNN\n",
        "cnn_image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# BERT tokenizer for questions\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Split train/test data\n",
        "train_single = [s for s in single_turn_samples if s[\"split\"] == \"train\"]\n",
        "test_single = [s for s in single_turn_samples if s[\"split\"] == \"test\"]\n",
        "\n",
        "print(f\"Single-Turn: Train={len(train_single)}, Test={len(test_single)}\")\n",
        "\n",
        "# Filter closed-ended questions for CNN-LSTM\n",
        "def filter_closed_ended(samples):\n",
        "    closed_answers = [\"yes\", \"no\", \"present\", \"absent\", \"normal\", \"abnormal\"]\n",
        "    filtered = [s for s in samples if s[\"answer\"].strip().lower() in closed_answers]\n",
        "    return filtered\n",
        "\n",
        "train_cnn_samples = filter_closed_ended(train_single)\n",
        "test_cnn_samples = filter_closed_ended(test_single)\n",
        "\n",
        "print(f\"Filtered Train CNN samples: {len(train_cnn_samples)}\")\n",
        "print(f\"Filtered Test CNN samples: {len(test_cnn_samples)}\")\n",
        "\n",
        "if len(train_cnn_samples) == 0 or len(test_cnn_samples) == 0:\n",
        "    print(\"Warning: No closed-ended answers found in your dataset. CNN-LSTM training will be skipped.\")\n",
        "else:\n",
        "    # Create SingleTurnDataset\n",
        "    train_cnn = SingleTurnDataset(\n",
        "        train_cnn_samples,\n",
        "        processor=cnn_image_transform,\n",
        "        tokenizer=bert_tokenizer,\n",
        "        max_seq_len=32\n",
        "    )\n",
        "    test_cnn = SingleTurnDataset(\n",
        "        test_cnn_samples,\n",
        "        processor=cnn_image_transform,\n",
        "        tokenizer=bert_tokenizer,\n",
        "        max_seq_len=32\n",
        "    )\n",
        "\n",
        "    # Prepare answer vocabulary\n",
        "    answer_vocab = list({s[\"answer\"].strip().lower() for s in train_cnn_samples})\n",
        "    num_classes = len(answer_vocab)\n",
        "    ans_to_idx = {a: i for i, a in enumerate(answer_vocab)}\n",
        "    idx_to_ans = {i: a for i, a in enumerate(answer_vocab)}\n",
        "\n",
        "    print(f\"Closed-Ended Answer Vocab: {answer_vocab}\")\n",
        "    print(f\"Number of Classes: {num_classes}\")\n",
        "\n",
        "    # Custom DataLoader for CNN-LSTM\n",
        "    class CNNDataLoader(DataLoader):\n",
        "        def __iter__(self):\n",
        "            for sample in self.dataset:\n",
        "                sample[\"ans_idx\"] = torch.tensor(ans_to_idx[sample[\"answer\"].strip().lower()])\n",
        "                yield sample\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_cnn_loader = CNNDataLoader(train_cnn, batch_size=8, shuffle=True)\n",
        "    test_cnn_loader = CNNDataLoader(test_cnn, batch_size=8, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qq0Hu3JavYYE",
        "outputId": "f59bd89e-8125-446a-b8ea-5bfa45153537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single-Turn: Train=0, Test=0\n",
            "Filtered Train CNN samples: 0\n",
            "Filtered Test CNN samples: 0\n",
            "Warning: No closed-ended answers found in your dataset. CNN-LSTM training will be skipped.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# --- 1. Load BLIP-1 processor and model ---\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip-image-captioning-base\",\n",
        "    torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# --- 2. Dataset classes ---\n",
        "class MultiTurnDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, samples, processor, max_seq_len=32, max_turns=3):\n",
        "        self.samples = samples\n",
        "        self.processor = processor\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.max_turns = max_turns\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.samples[idx]\n",
        "        img = s[\"image\"].resize((128, 128))\n",
        "        questions = s[\"questions\"][:self.max_turns]\n",
        "        answers = s[\"answers\"][:self.max_turns]\n",
        "\n",
        "        conversation = \"\"\n",
        "        for q, a in zip(questions[:-1], answers[:-1]):\n",
        "            conversation += f\"Q: {q} A: {a} \"\n",
        "        conversation += f\"Q: {questions[-1]} A:\"\n",
        "\n",
        "        # Process input text + image\n",
        "        inputs = self.processor(\n",
        "            images=s[\"image\"],\n",
        "            text=conversation,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=False,\n",
        "            truncation=True,\n",
        "            max_length=self.max_seq_len * self.max_turns\n",
        "        )\n",
        "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
        "\n",
        "        # Process labels\n",
        "        labels = self.processor.tokenizer(\n",
        "            answers[-1],\n",
        "            return_tensors=\"pt\",\n",
        "            padding=False,\n",
        "            truncation=True,\n",
        "            max_length=self.max_seq_len\n",
        "        )[\"input_ids\"].squeeze(0)\n",
        "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        inputs[\"labels\"] = labels\n",
        "        return inputs\n",
        "\n",
        "# --- 3. Manual collate_fn for variable-length batching ---\n",
        "def collate_fn(batch):\n",
        "    # Stack images\n",
        "    pixel_values = torch.stack([b[\"pixel_values\"] for b in batch])\n",
        "\n",
        "    # Pad input_ids, attention_mask, labels manually\n",
        "    input_ids = [b[\"input_ids\"] for b in batch]\n",
        "    attention_mask = [b[\"attention_mask\"] for b in batch]\n",
        "    labels = [b[\"labels\"] for b in batch]\n",
        "\n",
        "    # Find max lengths\n",
        "    max_len_input = max([x.size(0) for x in input_ids])\n",
        "    max_len_labels = max([x.size(0) for x in labels])\n",
        "\n",
        "    # Pad inputs\n",
        "    input_ids_padded = torch.stack([\n",
        "        torch.cat([x, x.new_zeros(max_len_input - x.size(0))]) for x in input_ids\n",
        "    ])\n",
        "    attention_mask_padded = torch.stack([\n",
        "        torch.cat([x, x.new_zeros(max_len_input - x.size(0))]) for x in attention_mask\n",
        "    ])\n",
        "    # Pad labels with -100\n",
        "    labels_padded = torch.stack([\n",
        "        torch.cat([x, x.new_full((max_len_labels - x.size(0),), -100)]) for x in labels\n",
        "    ])\n",
        "\n",
        "    return {\n",
        "        \"pixel_values\": pixel_values,\n",
        "        \"input_ids\": input_ids_padded,\n",
        "        \"attention_mask\": attention_mask_padded,\n",
        "        \"labels\": labels_padded\n",
        "    }\n",
        "\n",
        "# --- 4. Prepare datasets and loaders ---\n",
        "multi_turn_train_samples = [s for s in multi_turn_samples if s[\"split\"]==\"train\"]\n",
        "multi_turn_test_samples  = [s for s in multi_turn_samples if s[\"split\"]==\"test\"]\n",
        "\n",
        "train_multi_ds = MultiTurnDataset(multi_turn_train_samples, processor)\n",
        "test_multi_ds  = MultiTurnDataset(multi_turn_test_samples, processor)\n",
        "\n",
        "train_multi_loader = DataLoader(train_multi_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "test_multi_loader  = DataLoader(test_multi_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# --- 5. Training loop ---\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for batch in tqdm(train_multi_loader, desc=f\"Epoch {epoch+1} Multi-Turn Train\"):\n",
        "        batch = {k:v.to(device) for k,v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_multi_loader)\n",
        "    print(f\"Epoch {epoch+1} average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "# --- 6. Save model ---\n",
        "torch.save(model.state_dict(), \"blip_medvqa.pth\")\n",
        "print(\"\\nBLIP model saved as blip_medvqa.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "JmCDum2TxPhV",
        "outputId": "55dda66d-0d97-4d39-e7a1-01f49f1e6d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Multi-Turn Train:   0%|          | 0/9 [00:26<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected input batch_size (156) to match target batch_size (8).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2571536508.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_multi_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1} Multi-Turn Train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/blip/modeling_blip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, input_ids, attention_mask, output_attentions, output_hidden_states, labels, return_dict, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         \u001b[0mimage_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvision_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m         outputs = self.text_decoder(\n\u001b[0m\u001b[1;32m   1103\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/blip/modeling_blip_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, return_logits, is_decoder, reduction)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshifted_prediction_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m             \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m             \u001b[0mlm_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshifted_prediction_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"none\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m                 \u001b[0mlm_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m         \u001b[0;34m\"\"\"Runs the forward pass.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1385\u001b[0;31m         return F.cross_entropy(\n\u001b[0m\u001b[1;32m   1386\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3459\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (156) to match target batch_size (8)."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rQyiJx0EqEtj"
      }
    }
  ]
}